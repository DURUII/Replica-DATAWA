\section{Task Assignment}
\label{IV}
%{\color{brown}{
In this section, we introduce a task assignment component designed to address the ATA problem by planning a dynamically valid task sequence for all available workers. This component considers current and predicted tasks alongside worker availability windows to achieve optimal task assignments. Each worker's availability window reflects the time periods they are available for task assignments and can change dynamically due to factors such as breaks, shifts, or unforeseen circumstances.

We first present the Worker Dependency Separation (WDS) approach, which uses tree-based graph partitioning to divide workers into independent clusters and construct a hierarchical tree structure. A depth-first search algorithm is then employed to gather optimal results. Additionally, we incorporate a reinforcement learning approach to train a Task Value Function (TVF) based on these optimal results. Finally, we propose an adaptive algorithm for task assignment that adjusts the allocation of tasks to workers in response to changes in demand and supply, ensuring the most efficient task distribution.


\input{worker_dependency_separation}

\subsection{Task Value Function Learning}
%{\color{brown}{
Recursive search can often lead to significant computational complexity and instability. In Reinforcement Learning (RL), the value function evaluates the long-term value of a state-action pair, which is essential for guiding the agent in choosing the optimal action. To improve task assignment efficiency, we use a depth-first search method to gather training data and train a Task Value Function (TVF) based on this data.
%}}

%\begin{definition}[Reinforcement Learning]
In RL, the value function evaluates the long-term value of a state-action pair. It consists of three parts: state, action, and reward. In task assignment, the ``state" consists of the states of all remaining workers and tasks (i.e., locations and publication times).
The ``action" involves selecting a specific worker and determining a task sequence for them. 
The ``reward" represents the assigned number of tasks reflecting the effectiveness of the task assignment process. The state-action value function $\mathit{TVF}(st, at)$ represents the expected value of the cumulative reward that the agent may obtain in the future after performing action $at$ in state $st$:
\begin{equation}
\mathit{TVF}(st, at)=\mathbb{E}\left[V \mid ST=st, AT=at\right],
\end{equation}
%\end{definition}
where $V$ denotes the cumulative reward, and the expectation is taken over the distribution of possible future states and rewards.


We explain the depth-first search method, $\mathit{DFSearch}$, in Algorithm~\ref{dfs}. The algorithm starts by computing the remaining available workers $W_C$ of all nodes excluding $W_N$ in the tree (lines~\ref{dfs2}--\ref{dfs4}). If there are still workers to be probed, we will sequentially examine each available worker in $W_N$ (lines~\ref{dfs5}--\ref{dfs6}). We use depth-first-search to determine the optimal assignment number (line~\ref{dfs8}) and define the current state as $(W_N + W_C, S)$ and action as $(w, q)$, which corresponds to a RL process that appends current state, action, and reward (i.e., $st$, $at$, and $opt$) to training data $U$ (lines~\ref{dfs9}--\ref{dfs11}).
On the other hand, if all the workers have been considered, the algorithm will initiate the $\mathit{DFSearch}$ process on each child node of $N$ (lines~\ref{dfs15}--\ref{dfs16}).
\vspace{-0.2cm}
\begin{algorithm}
\small
    \label{dfs}
    \SetAlgoLined %显示end
    \caption{DFSearch}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    % 设置输入
    \Input{A node $N$, a set of tasks $S$, A set of workers $W_{N}$} 
    \Output{$opt$} 
    $opt \gets 0; W_C\gets \emptyset; U \gets \emptyset;$  \label{dfs1}\\
     \For{each child node $N_i$ of N}{ \label{dfs2}
        $W_C + \gets W_{N_i}$; \label{dfs3}
        } \label{dfs4}
    \eIf{$W_N \ne \emptyset$}{ \label{dfs5}
        \For{each worker $ w \in W_N$}{ \label{dfs6}
            \For{each sequence $ q \in Q_w$}{ \label{dfs7}
            $opt \gets max\{\mathit{DFSearch}(N, S-q, W_N - w)+|q|, opt\}$; \label{dfs8}\\
            $st \gets (W_N + W_C, S)$; \label{dfs9}\\
            $at \gets(w, q)$; \label{dfs10}\\
            $U + \gets (st, at, opt)$; \label{dfs11}\\
            } \label{dfs12}
        } \label{dfs13}
    }{ \label{dfs14}
        \For{each child node $N_i$ of N}{ \label{dfs15}
        $opt + \gets \mathit{DFSearch}(N_i, S, W_{N_i})$; \label{dfs16}
        } \label{dfs17}
    } \label{dfs18}
    \KwRet $opt$; \label{dfs19}
\end{algorithm}
\vspace{-0.5cm}

During learning, we apply Q-learning~\cite{watkins1992q} updates on samples (or mini-batches) of experience $\left(st, at, opt\right)$, drawn uniformly at random from the stored samples generated by $\mathit{DFSearch}$. The Q-learning update uses the following loss function:
\begin{equation}
L\left(\theta\right)=\mathbb{E}_{\left(st, at, opt\right) \sim U}\left[\left(opt-\mathit{TVF}\left(st, at ; \theta\right)\right)^2\right],
\end{equation}
where $\theta$ are the parameters of the TVF.


The memory consumption for storing all state-action pairs in Algorithm~\ref{dfs} is $O(|U| \times (|W| + |S| + |RS|))$, where $|U|$ represents the size of the training data $U$, and $|RS|$ denotes the average number of reachable tasks for each worker.


We then use the trained TVF to solve the assignment problem, referred to as $\mathit{DFSearch\_TVF}$, as outlined in Algorithm~\ref{rl}. The inputs include the root node $N$ of the sub-tree to be traversed, the remaining unassigned task set $S$, and the remaining available workers $W_N$ at node $N$. The output is the assignment result $\mathit{SA}$ within the sub-tree rooted at node $N$.

The algorithm begins by computing the remaining available workers, $W_C$, for all nodes in the tree, excluding $W_N$ (lines~\ref{rl2}--\ref{rl4}). If there are still workers to be probed, we will examine the first available worker in $W_N$ (lines~\ref{rl5}--\ref{rl6}). We define the current state as $\{W_N + W_C, S\}$ and the action as $\{w, q\}$, corresponding to reinforcement learning (RL). The value function TVF is then used to calculate the optimal task sequence $q_\mathit{best}$, which maximizes the reward from the valid task sequence set $Q_w$ (lines~\ref{rl7}--\ref{rl8}). We then append the best assigned result to output and recursively call the $\mathit{DFSearch\_TVF}$ approach by passing in the updated remaining task set $S-q_\mathit{best}$ and worker set $W_N-w$ (lines~\ref{rl9}--\ref{rl10}). Conversely, if all the workers have been counted, the algorithm will initiate the $\mathit{DFSearch\_TVF}$ process on each child node of $N$ (lines~\ref{rl12}--\ref{rl14}).

\vspace{-0.2cm}
\begin{algorithm}
\small
    \label{rl}
    \SetAlgoLined %显示end
    \caption{DFSearch\_TVF}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{A node $N$, a set of tasks $S$, A set of workers $W_{N}$}
    \Output{$\mathit{SA}$}
    $\mathit{SA} \gets \emptyset; W_C \gets \emptyset;$ \label{rl1}\\
     \For{each child node $N_i$ of N}{ \label{rl2}
        $W_C + \gets W_{N_i}$; \label{rl3}
        } \label{rl4}
    \eIf{$W_N \ne \emptyset$}{ \label{rl5}
        $w \gets W_N[0]$;  \label{rl6}\\
        $st \gets (W_N + W_C, S);$ \label{rl7}\\
        $q_{best} \gets argmax_{q \in Q_W}\mathit{TVF}(st, (w, q));$ \label{rl8}\\
        $\mathit{SA} + \gets \{w, q_{best}\};$ \label{rl9}\\
        $\mathit{SA} + \gets \mathit{DFSearch\_TVF}(N, S - q_{best},  W_N - w);$ \label{rl10}\\
    }{ \label{rl11}
        \For{each child node $N_i$ of N}{ \label{rl12}
        $\mathit{SA} + \gets \mathit{DFSearch\_TVF}(N_i, S, W_{N_i});$ \label{rl13}
        } \label{rl14}
    } \label{rl15}
    \KwRet $\mathit{SA}$; \label{rl16}
\end{algorithm}
\vspace{-0.5cm}
\subsection{Adaptive Algorithm}
In SC, tasks and workers are continuously changing and moving, requiring real-time updates and processing to ensure optimal assignment. This section describes the procedure of an adaptive algorithm that adjusts the task assignment based on current and predicted tasks, responding to fluctuations in supply and demand.

Algorithm~\ref{lta} outlines the complete process of the adaptive algorithm. The input is a continuous stream of arriving workers and tasks, and the output is the corresponding assignment result. When a worker or task appears on the platform, the optimal task planning assignment $\mathit{PA}$ is calculated by Algorithm~\ref{tpa} to achieve the global maximum revenue based on current and future tasks (lines~\ref{lta3}--\ref{lta9}). For each idle worker, the first task in their current planned sequence is executed (lines~\ref{lta10}--\ref{lta14}). Finally, we remove all workers and tasks %whose deadlines have passed, 
with past deadlines, 
as well as completed tasks (line~\ref{lta15}).
%\vspace{-0.2cm}
\begin{algorithm}\label{lta}
\small
    \SetAlgoLined %显示end
    \caption{Adaptive Algorithm}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    % 设置输入
    \Input{A stream of arriving objects $\{\delta_i|\delta_i \in \{w, s\} \}$}
    \Output{Task assignment $A$}
    $\mathit{PA} \gets \emptyset; A \gets \emptyset; W \gets \emptyset; S \gets \emptyset$; \label{lta1}\\
    \For{each arrive object $\delta$}{ \label{lta2}
    
        \eIf{$\delta$ is a worker}{ \label{lta3}
            $W \gets W + \delta$; \label{lta4}\\
            $\mathit{PA} \gets \mathit{TPA}(W, S)$; \label{lta5}
        }{ \label{lta6}
         $S \gets S + \delta$; \label{lta7}\\
        $\mathit{PA} \gets \mathit{TPA}(W, S)$;\label{lta8}
        }\label{lta9}
     
     \For{$\{w_i, \mathit{VR}(w_i)\} \in \mathit{PA}$}{ \label{lta10}
        \If{$w_i$ is ready to accept tasks and $len(\mathit{VR}(w_i)) \ge 1$}{ \label{lta11}
            $A.w_i \gets A.w_i + \mathit{VR}(w_i)[0]$; \label{lta12}
        }\label{lta13}
     }\label{lta14}
     Remove all workers/tasks whose deadlines have passed and completed tasks; \label{lta15}\\
    } \label{lta16}
   
    \KwRet $A$; \label{lta17}
\end{algorithm}
%\vspace{-0.5cm}


\begin{algorithm}
\small
    \label{tpa}
    \SetAlgoLined %显示end
    \caption{Task Planning Assignment (TPA) Algorithm}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    % 设置输入
    \Input{A set of workers $W$, a set of tasks $S$}
    \Output{$\mathit{PA}$}

    $opt \gets 0; \mathit{PA} \gets \emptyset;$  \label{tpa1}\\

    \For{each worker $w \in W$}{ \label{tpa2}
    $RS_w \gets$ compute the reachable tasks for $w$; \label{tpa3}\\
    $Q_w \gets$ compute the set of maximal valid task sequences for $w$; \label{tpa4}
    }\label{tpa5}
    $G \gets$ construct worker dependency graph; \label{tpa6}

    \For{each connected graph $g \in G$}{ \label{tpa7}
    $\mathbb{X}_g \gets$ decompose $g$ into vertex cluster; \label{tpa8}\\
    $N_g \gets$ organize $\mathbb{X}_g$ into a tree; \label{tpa9}\\
    $ \mathit{PA} + \gets \mathit{DFSearch\_TVF}(N_g, S, W_{N_g});$ \label{tpa10}
    } \label{tpa11}
    \KwRet $\mathit{PA}$; \label{tpa12}
    
\end{algorithm}

%\vspace{-0.5cm}
Next, we elaborate on the details of the task planning assignment in Algorithm~\ref{tpa}. The inputs are the worker set $W$ and the task set $S$. The output is the optimal task planning assignment $\mathit{PA}$, which aims to maximize the total expected revenue from both current and future tasks.
We compute the reachable task set $\mathit{RS}_w$ and the set of maximal valid task sequence $Q_w$ for each worker $w$ (lines~\ref{tpa3}--\ref{tpa4}), followed by constructing the worker dependency graph (line~\ref{tpa6}). Then, for each connected component $g \in G $, we decompose $g$ into a set of vertex clusters using the MCS algorithm (line~\ref{tpa8}) and organize them into a tree with the RTC algorithm (line~\ref{tpa9}). Finally, we apply the $\mathit{DFSearch\_TVF}$ algorithm to find the optimal assignment for each sub-problem (line~\ref{tpa10}). Finally, we can get the optimal task assignment (line~\ref{tpa12}).


